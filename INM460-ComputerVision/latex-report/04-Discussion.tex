\section{Discussion}
\label{Discussion-marker}
This coursework project had a variety of tasks which required a good level of organisation, from data pre-processing onwards. Therefore it also turned out an exercise in time management and resource allocation.  

Organising the image training sets, which seemed to be a minor task, turned out to be major, and provided an appreciation for what projects like ImageNet \cite{imagenet_cvpr09} must have been like.  

The original SURF paper \cite{Bay2008346} does account for RGB images being used, though the MATLAB implementation of detectSURFFeatures only accepts grayscale images. Future work with the HOG-SVM and SURF-SVM classifiers could include using RGB images to generate HOG and SURF features, by encoding each channel separately and to a fixed length channel vector as per single channel grayscale images, then stacking channels sequentially, such that the vector would always have the channel features in the same vector indices. 

One of SURF's strengths is finding descriptors invariant to orientation change, which we could have tested by not rotating images in our training set, then using rotated images in our test set. Under such conditions, we would expect that SURF-SVM might gain on the CNN and HOG-SVM classifiers.

We did not use the best possible SURF-SVM model for the sake of using a common set of images for both HOG-SVM and SURF-SVM.

The different accuracy obtained by testing on unseen individual images and unseen group images suggests overfitting may have occurred. We tested different splits for SURF-SVM models (which presented the lowest accuracy) of 80/20 and 70/30 without substantial change in accuracy observed for testing data. We assumed that, if all three classifiers - CNN HOG-SVM and SURF-SVM - were indeed overfitting, one cause may be there is a limited amount of variability that can be introduced by augmentation, and ending up with 200 sample images for every label may be the cause. Further splits would be required to determine if this was indeed the case. The expectation being, if an unorthodox split of 30/70 was tried and the accuracies for individual image classification were still high, this would confirm the hypothesis.  

Also, in terms of training/testing splits, K-fold cross-validation might help with establishing if the data was somehow impoverished by our augmentation.

Another cause of the model accuracy discrepancy between test individual and group images, may be due to group images not being representative enough. This may be a shortcoming of our processing pipeline, that might have benefited from trying to approximate individual and group image characteristics such as contrast.

We were not able to implement a confidence measure for HOG-SVM and SURF-SVM classifiers, so all labels generated by both classifiers have an ID, unlike our CNN classifier which shows an ID relative to a confidence measure.

Adding a confidence measure to HOG-SVM and SURF-SVM classifiers could be generated by using a measure of distance to the classification boundary. Further refinements, such as the use of multiple models and "voting" schemes could be one option, whereby the output labels from each model would be grouped and summed and the highest number chosen as the output label.

Producing the HOG-SVM and SURF-SVM classifiers was the highlight of this project and one of the highlights of my time at City so far, as it is the culmination of practical and theoretical knowledge obtained since joining the programme. The CNN classifier was expected to outperform both HOG-SVM and SURF-SVM. The fact that HOG-SVM matched the CNN classifier in performance and the SURF-SVM did quite well was a surprise in the true spirit of research, where the outcome is never certain.